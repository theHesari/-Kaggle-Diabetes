# -*- coding: utf-8 -*-
"""Copy of Kaggle-Diabetes

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eikKcpkCSae83SsJr0uDZ-VuP937Vdp8
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


#Read database by pandas library
df = pd.read_csv('/content/1689128937.csv')

#You can check first 5 rows of your database
df.head(5)

#For more information you can use .info()
df.info()

#You can plot and compare features by renaming Feature01 and Feature02
Feature01 = 'DiabetesPedigreeFunction'
Feature02 = 'Insulin'

plt.plot(df[df['Outcome']==0][Feature01], df[df['Outcome']==0][Feature02], 'b.')
plt.plot(df[df['Outcome']==1][Feature01], df[df['Outcome']==1][Feature02], 'r.')


#Lets create our model
"""**Logisic Regression without Preprocessing**"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score

#'Outcome' column is ur output, so we seprate tarining and testng columns
y = np.array(df['Outcome'])
X = np.array(df.drop('Outcome' , axis=1))

#Training / Validation dataset
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)

#Model
clf = LogisticRegression()
clf.fit(X_train, y_train)
print(clf.score(X_train, y_train))
print(clf.score(X_valid, y_valid))

#F-scores
f1s_train = []
f1s_valid = []
for c in [0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1, 2, 4, 8, 16]:
  clf = LogisticRegression(C=c)
  clf.fit(X_train, y_train)
  h_train = clf.predict(X_train)
  h_valid = clf.predict(X_valid)
  
  f1s_train.append(f1_score(y_train, h_train))
  f1s_valid.append(f1_score(y_valid, h_valid))
  
  print(c, '\tF-1 score valid:\t', f1_score(y_valid, h_valid))
  print(c, '\tF-1 score train:\t', f1_score(y_train, h_train))
  print('---------------------------------------------------')


#Plot F-scores for training and validation data and compare them
plt.plot(f1s_train, 'bo--', label='Train')
plt.plot(f1s_valid, 'r*--', label='Valid')
plt.legend()


"""**Logisic Regression with Preprocessing**"""

from sklearn.impute import SimpleImputer

#Preprocessing : Relace any 0 value in dataset with the mean value of the column except the real 0 values (Pregnancies, Outcome)
imp_mean = SimpleImputer(missing_values=0)
df_impute = imp_mean.fit_transform(df)
df_impute[df['Pregnancies']==0, 0] = 0
df_impute[df['Outcome']==0, 8] = 0

#Training / Validation dataset
X_train, X_valid, y_train, y_valid = train_test_split(df_impute[:, :7], df_impute[:, 8].astype(int), test_size=0.2)

#F-scores
f1s_train = []
f1s_valid = []
for c in [0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1, 2, 4, 8, 16]:
  clf = LogisticRegression(C=c)
  clf.fit(X_train, y_train)
  h_train = clf.predict(X_train)
  h_valid = clf.predict(X_valid)
  
  f1s_train.append(f1_score(y_train, h_train))
  f1s_valid.append(f1_score(y_valid, h_valid))
  
  print(c, '\tF-1 score valid:\t', f1_score(y_valid, h_valid))
  print(c, '\tF-1 score train:\t', f1_score(y_train, h_train))
  print('---------------------------------------------------')

#Plot F-scores for training and validation data and compare them
plt.plot(f1s_train, 'bo--', label='Train')
plt.plot(f1s_valid, 'r*--', label='Valid')
plt.legend()
